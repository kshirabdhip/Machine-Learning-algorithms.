{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "test = dataset.data[100:150]\n",
    "articles = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day'], ['It', 'sports', 'car', 'looked', 'late', 'early', '70s'], ['It', 'called', 'Bricklin'], ['The', 'doors', 'really', 'small'], ['In', 'addition', 'front', 'bumper', 'separate', 'rest', 'body'], ['This', 'know'], ['If', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please']]\n"
     ]
    }
   ],
   "source": [
    "corpus =[]\n",
    "for doc in articles:\n",
    "    d =[]\n",
    "    for sent in nltk.sent_tokenize(doc):\n",
    "        s =[]\n",
    "        sentTag=nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "        for word,tag in sentTag:\n",
    "            try:\n",
    "                re.findall(r\"[N]\\w+|[V]\\w+|[A-Za-z]\\w+\",tag)[0]\n",
    "                if word not in stop_words and len(word) >1 and re.findall(r\"(?:^|(?<= ))[a-zA-Z0-9]+(?= |$)\",word)[0]:\n",
    "                    s.append(word)\n",
    "            except:\n",
    "                pass\n",
    "        d.append(s)\n",
    "    corpus.append(d)\n",
    "\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for quick model check let's take only 100 rows of data\n",
    "corpus = corpus[0:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.corpus import movie_reviews as data\n",
    "corpus =[]\n",
    "for fileid in data.fileids()[0:50]:\n",
    "    d = []\n",
    "    for sent in data.sents(fileid):\n",
    "        s = []\n",
    "        word_tag = nltk.pos_tag(sent)\n",
    "        for word,tag in word_tag:\n",
    "            try:\n",
    "                re.findall(r\"[N]\\w+|[V]\\w+\",tag)[0]\n",
    "                if word not in stop_words and len(word) >1 and re.findall(r\"(?:^|(?<= ))[a-zA-Z0-9]+(?= |$)\",word)[0]:\n",
    "                    s.append(word)\n",
    "            except:\n",
    "                pass\n",
    "        d.append(s)\n",
    "    corpus.append(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"regulation.csv\")\n",
    "#text = df.content.values[0:7]\n",
    "\n",
    "##corpus = []\n",
    "#for doc in text:\n",
    "#    corpus.append([line.split()for line in doc.split(\".\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary  = [word for doc in corpus for line in doc for word in line]\n",
    "vocabulary =list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Made',\n",
       " 'minimal',\n",
       " 'PostScript',\n",
       " 'organized',\n",
       " 'malfunction',\n",
       " 'Whether',\n",
       " 'Pat',\n",
       " 'amps',\n",
       " 'Europe',\n",
       " 'opposing']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing word \n",
    "id_word = {i:w for i,w in enumerate(vocabulary)}\n",
    "word_id  = {w:i for i,w in enumerate(vocabulary)}\n",
    "\n",
    "# corpus with word id insted of word\n",
    "docs = []\n",
    "for doc in corpus:\n",
    "    docs.append ([[word_id[word] for word in line] for line in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters:\n",
    "K_gl = 5 # number of global topic \n",
    "K_loc = 5 # number of local topic\n",
    "\n",
    "gamma         = 0.1 #parameter of sentence prior\n",
    "alpha_gl      = 0.1 # parameter of global topics prior\n",
    "alpha_loc     = 0.1 # parameter of local topics prior\n",
    "\n",
    "#parameters for choosing between local and global topics\n",
    "alpha_mix_gl  = 0.1\n",
    "alpha_mix_loc = 0.1\n",
    "\n",
    "beta_gl = 0.1 # parameter of global words prior\n",
    "beta_loc = 0.1 # parameter of local words prior\n",
    "\n",
    "T = 3 # sliding window width\n",
    "\n",
    "docs = docs\n",
    "W = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random fitting to initialize\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Kgl word distributions for global topics - list(list(list(word from sent)sent from doc)doc from corpus)\n",
    "v_d_s_n     = [] #sumi - \n",
    "r_d_s_n     = [] #sumi - global or local\n",
    "z_d_s_n     = [] #sumi- word -topic dist\n",
    "\n",
    "#topic distribution over words\n",
    "#n prefix is for count\n",
    "n_gl_z_w    = np.zeros((K_gl, W)) #zero initialized matrix of (num. global topic X no of words)\n",
    "n_gl_z      = np.zeros(K_gl) # zero initialized list, len of list = number of topics.number of words in each topic\n",
    "n_d_s_v     = [] #sumi\n",
    "n_d_s       = [] #sumi\n",
    "n_d_v_gl    = [] #sumi\n",
    "n_d_v       = [] #sumi\n",
    "\n",
    "# topic dist over doc\n",
    "n_d_gl_z    = np.zeros((len(docs), K_gl)) #zero initialized matrix of (num.of doc X no globak topic)\n",
    "n_d_gl      = np.zeros((len(docs))) # zero initialized list,  number of doc.\n",
    "\n",
    "#Kloc word distributions for local topics\n",
    "n_loc_z_w   = np.zeros((K_loc, W)) # matrix of size (no of local topic X vocabulary size)\n",
    "n_loc_z     = np.zeros(K_loc) # number of local topic\n",
    "n_d_v_loc   = [] #sumi\n",
    "n_d_v_loc_z = [] #sumi\n",
    "\n",
    "inflation = 0\n",
    "\n",
    "print (\"random fitting to initialize\")\n",
    "for m, doc in enumerate(docs):\n",
    "    v_d = []\n",
    "    r_d = []\n",
    "    z_d = []\n",
    "\n",
    "    n_d_s_v_d = []\n",
    "    n_d_s_d = []\n",
    "        \n",
    "    n_d_v_gl_v = [inflation for v in range(T+len(doc)-1)]\n",
    "    n_d_v_v = [inflation for v in range(T+len(doc)-1)]\n",
    "    n_d_v_loc_v = [inflation for v in range(T+len(doc)-1)]\n",
    "    n_d_v_loc_z_v  = [[inflation for k in range(K_loc)] for v in range(T+len(doc)-1)]\n",
    "\n",
    "    n_d_v_gl.append(n_d_v_gl_v)\n",
    "    n_d_v.append(n_d_v_v)\n",
    "    n_d_v_loc.append(n_d_v_loc_v)\n",
    "    n_d_v_loc_z.append(n_d_v_loc_z_v)\n",
    "       \n",
    "    for s, sent in enumerate(doc):\n",
    "        v_s = [np.random.randint(0, T) for i, word in enumerate(sent)]\n",
    "        r_s = [\"gl\" if (np.random.randint(0, 2) == 0 )else \"loc\" for i, word in enumerate(sent)]\n",
    "        z_s = [np.random.randint(0, K_gl)if r ==\"gl\" else np.random.randint(0, K_loc) for r in r_s]\n",
    "        n_d_s_v_s = [inflation for v in range(T)]\n",
    "        \n",
    "        v_d.append(v_s)\n",
    "        r_d.append(r_s)\n",
    "        z_d.append(z_s)\n",
    "        \n",
    "        n_d_s_v_d.append(n_d_s_v_s)\n",
    "        n_d_s_d.append(inflation) # initialize n_d_s   \n",
    "\n",
    "    v_d_s_n.append(v_d)\n",
    "    r_d_s_n.append(r_d)\n",
    "    z_d_s_n.append(z_d)\n",
    "    \n",
    "    n_d_s_v.append(n_d_s_v_d)\n",
    "    n_d_s.append(n_d_s_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize\n",
      "init comp.\n"
     ]
    }
   ],
   "source": [
    "print (\"initialize\")\n",
    "\n",
    "for m, doc in enumerate(docs):\n",
    "    \n",
    "    for s, sent in enumerate(doc):\n",
    "        for i, word in enumerate(sent):\n",
    "            v = v_d_s_n[m][s][i] # 0--T (v_d_s_n - A word can be sampled using any window covering its sentence s)\n",
    "            r = r_d_s_n[m][s][i] # global or local topic\n",
    "            z = z_d_s_n[m][s][i] # assigned global topic to the word\n",
    "\n",
    "            \n",
    "            if r == \"gl\":\n",
    "                n_gl_z_w[z][word]      += 1\n",
    "                n_gl_z[z]              += 1\n",
    "                n_d_v_gl[m][s+v]       += 1\n",
    "                n_d_gl_z[m][z]         += 1\n",
    "                n_d_gl[m]              += 1\n",
    "\n",
    "            elif r == \"loc\":\n",
    "                n_loc_z_w[z][word]     += 1\n",
    "                n_loc_z[z]             += 1\n",
    "                n_d_v_loc[m][s+v]      += 1\n",
    "                n_d_v_loc_z[m][s+v][z] += 1\n",
    "\n",
    "print (\"init comp.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_gl  = (n_gl_z_w + 1) / (n_gl_z[:, np.newaxis] + 1) # if num of topic(k) =2 then 2 vector will be generated\n",
    "\n",
    "phi_loc = (n_loc_z_w + 1) / (n_loc_z[:, np.newaxis] + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v_d_s_n - sample word from sentence using any window which covers it.\n",
    "#r_d_s_n - each word belong to global or local topic\n",
    "#z_d_s_n - the word belong to which topic cluster of local or global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "global topic: 0 (1006 words)\n",
      "['The', 'people', 'use', 'one', 'get', 'Russian', 'know', 'Reserve', 'year', 'think', 'part', 'find', 'It', 'say', 'Armenians', 'like', 'high', 'would', 'windows', 'genocide']\n",
      "\n",
      "global topic: 1 (1070 words)\n",
      "['The', 'This', 'know', 'one', 'people', 'GPF', 'using', 'As', 'way', '180', 'like', 'State', 'Jews', 'would', 'board', 'almost', 'Armenians', 'We', 'No', 'may']\n",
      "\n",
      "global topic: 2 (1045 words)\n",
      "['The', 'would', 'think', 'Armenian', 'Russian', 'data', 'may', 'memory', '1st', 'Turkish', 'like', 'major', 'fatwa', 'Reserve', 'day', 'available', 'get', 'options', 'Information', 'used']\n",
      "\n",
      "global topic: 3 (1096 words)\n",
      "['The', 'It', 'would', 'could', 'Armenian', 'Armenians', 'This', 'think', 'used', 'After', 'No', 'might', 'As', 'That', 'like', 'car', 'tickets', 'Mac', 'application', 'left']\n",
      "\n",
      "global topic: 4 (1071 words)\n",
      "['The', 'people', 'know', 'one', 'Armenian', 'would', 'You', 'Naval', 'fact', 'many', 'Titan', 'steam', 'Armenians', 'number', 'need', 'This', 'It', 'He', 'collaboration', 'get']\n",
      "\n",
      "******             ************                *************\n",
      "\n",
      "local topic: 0 (1131 words)\n",
      "['The', 'No', 'use', 'enough', 'car', 'would', 'one', 'case', 'could', 'system', 'request', 'Why', 'pp', 'even', 'data', 'It', 'information', 'shared', 'without', 'He']\n",
      "\n",
      "local topic: 1 (1073 words)\n",
      "['one', 'The', 'get', 'would', 'could', 'use', 'No', 'people', 'It', 'Jews', 'two', 'When', 'problem', 'My', 'email', 'Thanks', 'heard', 'available', 'years', 'need']\n",
      "\n",
      "local topic: 2 (1063 words)\n",
      "['The', 'know', 'still', 'anyone', 'wrong', 'number', 'No', 'years', 'like', 'think', 'If', 'insurance', 'used', 'another', 'And', 'It', 'well', 'info', 'back', 'seems']\n",
      "\n",
      "local topic: 3 (1171 words)\n",
      "['No', 'year', 'The', '24', 'new', 'one', 'people', 'first', 'Turkish', 'would', 'In', 'system', 'gas', 'engine', 'Hultman', 'currently', 'right', 'several', 'Please', 'Reserve']\n",
      "\n",
      "local topic: 4 (1114 words)\n",
      "['The', 'would', 'one', 'people', 'Armenian', 'No', 'could', 'get', 'It', 'You', 'rate', 'use', 'many', 'Russian', 'high', 'years', 'better', 'Turks', 'Cassini', 'provide']\n",
      "_________________________________________________________________\n",
      "5\n",
      "\n",
      "global topic: 0 (1138 words)\n",
      "['The', 'people', 'use', 'one', 'get', 'Russian', 'know', 'Reserve', 'year', 'think', 'part', 'find', 'It', 'say', 'Armenians', 'like', 'high', 'would', 'windows', 'genocide']\n",
      "\n",
      "global topic: 1 (1063 words)\n",
      "['The', 'This', 'know', 'one', 'people', 'GPF', 'using', 'As', 'way', '180', 'like', 'State', 'Jews', 'would', 'board', 'almost', 'Armenians', 'We', 'No', 'may']\n",
      "\n",
      "global topic: 2 (1068 words)\n",
      "['The', 'would', 'think', 'Armenian', 'Russian', 'data', 'may', 'memory', '1st', 'Turkish', 'like', 'major', 'fatwa', 'Reserve', 'day', 'available', 'get', 'options', 'Information', 'used']\n",
      "\n",
      "global topic: 3 (1194 words)\n",
      "['The', 'It', 'would', 'could', 'Armenian', 'Armenians', 'This', 'think', 'used', 'After', 'No', 'might', 'As', 'That', 'like', 'car', 'tickets', 'Mac', 'application', 'left']\n",
      "\n",
      "global topic: 4 (1141 words)\n",
      "['The', 'people', 'know', 'one', 'Armenian', 'would', 'You', 'Naval', 'fact', 'many', 'Titan', 'steam', 'Armenians', 'number', 'need', 'This', 'It', 'He', 'collaboration', 'get']\n",
      "\n",
      "******             ************                *************\n",
      "\n",
      "local topic: 0 (1136 words)\n",
      "['The', 'No', 'use', 'enough', 'car', 'would', 'one', 'case', 'could', 'system', 'request', 'Why', 'pp', 'even', 'data', 'It', 'information', 'shared', 'without', 'He']\n",
      "\n",
      "local topic: 1 (967 words)\n",
      "['one', 'The', 'get', 'would', 'could', 'use', 'No', 'people', 'It', 'Jews', 'two', 'When', 'problem', 'My', 'email', 'Thanks', 'heard', 'available', 'years', 'need']\n",
      "\n",
      "local topic: 2 (1045 words)\n",
      "['The', 'know', 'still', 'anyone', 'wrong', 'number', 'No', 'years', 'like', 'think', 'If', 'insurance', 'used', 'another', 'And', 'It', 'well', 'info', 'back', 'seems']\n",
      "\n",
      "local topic: 3 (947 words)\n",
      "['No', 'year', 'The', '24', 'new', 'one', 'people', 'first', 'Turkish', 'would', 'In', 'system', 'gas', 'engine', 'Hultman', 'currently', 'right', 'several', 'Please', 'Reserve']\n",
      "\n",
      "local topic: 4 (1141 words)\n",
      "['The', 'would', 'one', 'people', 'Armenian', 'No', 'could', 'get', 'It', 'You', 'rate', 'use', 'many', 'Russian', 'high', 'years', 'better', 'Turks', 'Cassini', 'provide']\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# for inference\n",
    "for ii in range(10): # number of iteration\n",
    "    for m, doc in enumerate(docs):\n",
    "        for s, sent in enumerate(doc):\n",
    "            for i, word in enumerate(sent):\n",
    "                v = v_d_s_n[m][s][i] # 0--T\n",
    "                r = r_d_s_n[m][s][i]\n",
    "                z = z_d_s_n[m][s][i]\n",
    "\n",
    "                # discount for i-th word t with topic z\n",
    "                if r == \"gl\":\n",
    "                    n_gl_z_w[z][word]      -= 1\n",
    "                    n_gl_z[z]              -= 1\n",
    "                    n_d_v_gl[m][s+v]       -= 1\n",
    "                    n_d_gl_z[m][z]         -= 1\n",
    "                    n_d_gl[m]              -= 1\n",
    "                else:\n",
    "                    n_loc_z_w[z][word]     -= 1\n",
    "                    n_loc_z[z]             -= 1\n",
    "                    n_d_v_loc[m][s+v]      -= 1\n",
    "                    n_d_v_loc_z[m][s+v][z] -= 1\n",
    "\n",
    "                # sampling topic new_z for t\n",
    "                p_v_r_z = []\n",
    "                label_v_r_z = []\n",
    "                for v_t in range(T):\n",
    "                    # for r == \"gl\"\n",
    "                    for z_t in range(K_gl):\n",
    "                        label = [v_t, \"gl\", z_t]\n",
    "                        label_v_r_z.append(label)\n",
    "                        # sampling eq as gl\n",
    "                        term1 = float(n_gl_z_w[z_t][word] + beta_gl) / (n_gl_z[z_t] + W*beta_gl) # P(w|r, z)\n",
    "                        term2 = float(n_d_s_v[m][s][v_t] + gamma) / (n_d_s[m][s] + T*gamma) #P(v)\n",
    "                        term3 = float(n_d_v_gl[m][s+v_t] + alpha_mix_gl) / (n_d_v[m][s+v_t] + alpha_mix_gl + alpha_mix_loc) #P(r|v)\n",
    "                        term4 = float(n_d_gl_z[m][z_t] + alpha_gl) / (n_d_gl[m] + K_gl*alpha_gl) #P(z|r, v)\n",
    "                        score = term1 * term2 * term3 * term4\n",
    "                        p_v_r_z.append(score)\n",
    "                    # for r == \"loc\" \n",
    "                    for z_t in range(K_loc):\n",
    "                        label = [v_t, \"loc\", z_t]\n",
    "                        label_v_r_z.append(label)\n",
    "                        # sampling eq as loc\n",
    "                        term1 = float(n_loc_z_w[z_t][word] + beta_loc) / (n_loc_z[z_t] + W*beta_loc) # P(w|r, z)\n",
    "                        term2 = float(n_d_s_v[m][s][v_t] + gamma) / (n_d_s[m][s] + T*gamma)#P(v)\n",
    "                        term3 = float(n_d_v_loc[m][s+v_t] + alpha_mix_loc) / (n_d_v[m][s+v_t] + alpha_mix_gl + alpha_mix_loc)#P(r|v)\n",
    "                        term4 = float(n_d_v_loc_z[m][s+v_t][z_t] + alpha_loc) / (n_d_v_loc[m][s+v_t] + K_loc*alpha_loc)#P(z|r, v)\n",
    "                        score = term1 * term2 * term3 * term4\n",
    "                        p_v_r_z.append(score)\n",
    "\n",
    "                np_p_v_r_z = np.array(p_v_r_z)\n",
    "\n",
    "                new_p_v_r_z_idx = np.random.multinomial(1, np_p_v_r_z / np_p_v_r_z.sum()).argmax()\n",
    "                new_v, new_r, new_z = label_v_r_z[new_p_v_r_z_idx]\n",
    "\n",
    "                # update the new topic assignment of word\n",
    "                if new_r == \"gl\":\n",
    "                    n_gl_z_w[new_z][word]          += 1\n",
    "                    n_gl_z[new_z]                  += 1\n",
    "                    n_d_v_gl[m][s+new_v]           += 1\n",
    "                    n_d_gl_z[m][new_z]             += 1\n",
    "                    n_d_gl[m]                      += 1\n",
    "                else :\n",
    "                    n_loc_z_w[new_z][word]         += 1\n",
    "                    n_loc_z[new_z]                 += 1\n",
    "                    n_d_v_loc[m][s+new_v]          += 1\n",
    "                    n_d_v_loc_z[m][s+new_v][new_z] += 1\n",
    "\n",
    "\n",
    "                v_d_s_n[m][s][i] = new_v\n",
    "                r_d_s_n[m][s][i] = new_r\n",
    "                z_d_s_n[m][s][i] = new_z\n",
    "    \n",
    "    #update other parameters\n",
    "    #output_word_topic_dist\n",
    "    z_gl_count = np.zeros(K_gl, dtype=int)\n",
    "    z_loc_count = np.zeros(K_loc, dtype=int)\n",
    "    word_gl_count = [dict() for k in range(K_gl)]\n",
    "    word_loc_count = [dict() for k in range(K_loc)]\n",
    "    for m, doc in enumerate(docs):\n",
    "        for s, sent in enumerate(doc):\n",
    "            for i, word in enumerate(sent):\n",
    "#               v = mglda.v_d_s_n[m][s][i] # 0--T\n",
    "                r = r_d_s_n[m][s][i]\n",
    "                z = z_d_s_n[m][s][i]\n",
    "                if r == \"gl\":\n",
    "                    z_gl_count[z] += 1\n",
    "                    if word in word_gl_count[z]:\n",
    "                        word_gl_count[z][word]  += 1\n",
    "                    else:\n",
    "                        word_gl_count[z][word]   = 1\n",
    "                elif r == \"loc\":\n",
    "                    z_loc_count[z] += 1\n",
    "                    if word in word_loc_count[z]:\n",
    "                        word_loc_count[z][word] += 1\n",
    "                    else:\n",
    "                        word_loc_count[z][word]  = 1\n",
    "                else:\n",
    "                    print (\"error3: \" + str(r))\n",
    "    if ii% 5 == 0:\n",
    "        print(ii)\n",
    "        for k in range(K_gl):\n",
    "            print (\"\\nglobal topic: %d (%d words)\" % (k, z_gl_count[k]))\n",
    "            print([vocabulary[w] for w in np.argsort(-phi_gl[k])[:20]])\n",
    "        print(\"\\n******             ************                *************\")\n",
    "\n",
    "        for k in range(K_loc):\n",
    "            print (\"\\nlocal topic: %d (%d words)\" % (k, z_loc_count[k]))\n",
    "            print([vocabulary[w] for w in np.argsort(-phi_loc[k])[:20]])\n",
    "        print(\"_________________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
